{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969706bb-bbfb-4bfe-83b4-b210a6acd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, os\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import threading\n",
    "import OpenVisus as ov\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from dask import delayed\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dask_jobqueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b1c20-ce7a-4fdc-8d92-013b143fcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import performance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020699a8-3db0-44f5-b53a-e7ef6be9205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## File paths ################\n",
    "lustre_scratch    = \"/glade/work/dpanta\"\n",
    "era5_surface_data = \"/gdex/data/special_projects/harshah/ARCO/e5.oper.an.sfc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_YEAR = 1940  \n",
    "\n",
    "def _is_leap(y: int) -> bool:\n",
    "    return (y % 4 == 0) and (y % 100 != 0 or y % 400 == 0)\n",
    "\n",
    "def _leap_days_since_base(year: int) -> int:\n",
    "    return sum(_is_leap(y) for y in range(BASE_YEAR, year))\n",
    "\n",
    "def hour_index_from_iso_big(iso: str) -> int:\n",
    "    s = iso.strip().replace(\"-\", \" \")\n",
    "    parts = s.split()\n",
    "    if len(parts) == 2:\n",
    "        date, hh = parts\n",
    "        ts = pd.to_datetime(f\"{date} {hh}:00:00\", utc=True)\n",
    "    elif len(parts) == 1:\n",
    "        ts = pd.to_datetime(parts[0] + \" 00:00:00\", utc=True)\n",
    "    else:\n",
    "        ts = pd.to_datetime(iso.replace(\"-\", \" \", 3).replace(\" \", \"T\", 1), utc=True)\n",
    "\n",
    "    y = ts.year\n",
    "    year_start = pd.Timestamp(year=y, month=1, day=1, tz=\"UTC\")\n",
    "    hours_since_year_start = int((ts - year_start) / pd.Timedelta(hours=1))\n",
    "    leap_days = _leap_days_since_base(y)\n",
    "    return y*365*24 + leap_days*24 + hours_since_year_start + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530f82a-5309-4121-a34e-d9fcda1fdc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = PBSCluster(\n",
    "        job_name = 'dask-osdf-25',\n",
    "        cores = 1,\n",
    "        memory = '4GiB',\n",
    "        processes = 1,\n",
    "        local_directory = lustre_scratch + '/dask/spill',\n",
    "        log_directory = lustre_scratch + '/dask/logs/',\n",
    "        resource_spec = 'select=1:ncpus=1:mem=4GB',\n",
    "        queue = 'casper',\n",
    "        account='P43713000',\n",
    "        walltime = '3:00:00',\n",
    "        #interface = 'ib0'\n",
    "        interface = 'ext'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95bd7f-6d6f-4b66-89ec-39518cbb9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "# Scale the cluster and display cluster dashboard URL\n",
    "n_workers =5\n",
    "cluster.scale(n_workers)\n",
    "client.wait_for_workers(n_workers = n_workers)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_YEAR = 1940\n",
    "def _is_leap(y: int) -> bool:\n",
    "    return (y % 4 == 0) and (y % 100 != 0 or y % 400 == 0)\n",
    "def _leap_days_since_base(year: int) -> int:\n",
    "    return sum(_is_leap(y) for y in range(BASE_YEAR, year))\n",
    "def hour_index_from_iso_big(iso: str) -> int:\n",
    "    s = iso.strip().replace(\"-\", \" \")\n",
    "    parts = s.split()\n",
    "    if len(parts) == 2:\n",
    "        date, hh = parts\n",
    "        ts = pd.to_datetime(f\"{date} {hh}:00:00\", utc=True)\n",
    "    elif len(parts) == 1:\n",
    "        ts = pd.to_datetime(parts[0] + \" 00:00:00\", utc=True)\n",
    "    else:\n",
    "        ts = pd.to_datetime(iso.replace(\"-\", \" \", 3).replace(\" \", \"T\", 1), utc=True)\n",
    "    y = ts.year\n",
    "    year_start = pd.Timestamp(year=y, month=1, day=1, tz=\"UTC\")\n",
    "    hours_since_year_start = int((ts - year_start) / pd.Timedelta(hours=1))\n",
    "    leap_days = _leap_days_since_base(y)\n",
    "    return y*365*24 + leap_days*24 + hours_since_year_start + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c798e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_PATH = \"/glade/work/dpanta/era5/idx/2T/era5_sfc_2T_zip.idx\"\n",
    "\n",
    "H, W = 721, 1440\n",
    "lat = np.linspace(90.0, -90.0, H, dtype=np.float32)\n",
    "W1D = np.cos(np.deg2rad(lat)).astype(np.float32)\n",
    "FULL_DEN = float(W1D.sum() * W)\n",
    "\n",
    "_DB = None\n",
    "_ACCESS = None\n",
    "\n",
    "def _get_db_and_access():\n",
    "    global _DB, _ACCESS\n",
    "    if _DB is None:\n",
    "        _DB = ov.LoadDataset(IDX_PATH)\n",
    "        try:\n",
    "            _ACCESS = _DB.createAccess()\n",
    "        except Exception:\n",
    "            _ACCESS = None\n",
    "    return _DB, _ACCESS\n",
    "\n",
    "def weighted_global_mean_fast(a: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if np.isfinite(a).all():\n",
    "        row_sum = a.sum(axis=1, dtype=np.float32)\n",
    "        return float(np.dot(W1D, row_sum) / FULL_DEN)\n",
    "    row_sum = np.nansum(a, axis=1).astype(np.float32)\n",
    "    row_cnt = np.sum(np.isfinite(a), axis=1, dtype=np.int32)\n",
    "    den = float(np.dot(W1D, row_cnt.astype(np.float32)))\n",
    "    return float(np.dot(W1D, row_sum) / den) if den > 0 else np.nan\n",
    "\n",
    "def gmst_hours_block_threaded(t0: int, n_hours: int, max_threads: int = 4) -> list[float]:\n",
    "    db, access = _get_db_and_access()\n",
    "\n",
    "    def read_reduce(t: int) -> float:\n",
    "        if access is not None:\n",
    "            a = db.read(time=t, access=access)\n",
    "        else:\n",
    "            a = db.read(time=t)\n",
    "        return weighted_global_mean_fast(a)\n",
    "\n",
    "    out = [None] * n_hours\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as ex:\n",
    "        futs = {ex.submit(read_reduce, t0 + k): k for k in range(n_hours)}\n",
    "        for f in as_completed(futs):\n",
    "            k = futs[f]\n",
    "            out[k] = f.result()\n",
    "    return out\n",
    "\n",
    "YEAR = 1949\n",
    "start_idx = hour_index_from_iso_big(f\"{YEAR}-01-01 00\")\n",
    "end_idx   = hour_index_from_iso_big(f\"{YEAR+1}-01-01 00\")\n",
    "total_hours = end_idx - start_idx\n",
    "\n",
    "BLOCK_HOURS = 24 * 30  \n",
    "n_blocks = (total_hours + BLOCK_HOURS - 1) // BLOCK_HOURS\n",
    "\n",
    "tasks = []\n",
    "for b in range(n_blocks):\n",
    "    b_start = start_idx + b * BLOCK_HOURS\n",
    "    b_n = min(BLOCK_HOURS, total_hours - b * BLOCK_HOURS)\n",
    "    tasks.append(delayed(gmst_hours_block_threaded)(b_start, b_n, max_threads=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 2 -r 3 \n",
    "blocks = dask.compute(*tasks)\n",
    "gmst_vals = np.fromiter((v for block in blocks for v in block), dtype=np.float32, count=total_hours)\n",
    "print(\"Annual GMST\", YEAR, \":\", float(np.nanmean(gmst_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c619de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import numpy as np, dask\n",
    "from dask import delayed\n",
    "import OpenVisus as ov\n",
    "\n",
    "IDX_PATH = \"/glade/work/dpanta/era5/idx/2T/era5_sfc_2T_zip.idx\"\n",
    "QUALITY  = -2  # half-res\n",
    "\n",
    "_DB = None\n",
    "_ACCESS = None        \n",
    "_W1D = None\n",
    "_FULL_DEN = None\n",
    "\n",
    "def _get_db_and_access():\n",
    "    \"\"\"Match your API; returns a DB and a (unused) process-wide access handle.\"\"\"\n",
    "    global _DB, _ACCESS\n",
    "    if _DB is None:\n",
    "        _DB = ov.LoadDataset(IDX_PATH)\n",
    "        try:\n",
    "            _ACCESS = _DB.createAccess()\n",
    "        except Exception:\n",
    "            _ACCESS = None\n",
    "    return _DB, _ACCESS\n",
    "\n",
    "def _init_weights_for_quality(example_time: int):\n",
    "    \"\"\"Read one QUALITY frame to discover (H,W) and build matching weights/den.\"\"\"\n",
    "    global _W1D, _FULL_DEN\n",
    "    if _W1D is not None and _FULL_DEN is not None:\n",
    "        return\n",
    "    db, acc = _get_db_and_access()\n",
    "    a0 = db.read(time=example_time, quality=QUALITY, access=acc) if acc is not None else db.read(time=example_time, quality=QUALITY)\n",
    "    H, W = a0.shape[-2], a0.shape[-1]\n",
    "    lat = np.linspace(90.0, -90.0, H, dtype=np.float32)\n",
    "    _W1D = np.cos(np.deg2rad(lat)).astype(np.float32)\n",
    "    _FULL_DEN = float(_W1D.sum() * W)\n",
    "\n",
    "def weighted_global_mean_fast(a: np.ndarray) -> float:\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    if np.isfinite(a).all():\n",
    "        row_sum = a.sum(axis=1, dtype=np.float32)\n",
    "        return float(np.dot(_W1D, row_sum) / _FULL_DEN)\n",
    "    row_sum = np.nansum(a, axis=1).astype(np.float32)\n",
    "    row_cnt = np.sum(np.isfinite(a), axis=1, dtype=np.int32).astype(np.float32)\n",
    "    den = float(np.dot(_W1D, row_cnt))\n",
    "    return float(np.dot(_W1D, row_sum) / den) if den > 0 else np.nan\n",
    "\n",
    "# ---- thread-local access so parallel reads don’t clash ----\n",
    "_tlocal = threading.local()\n",
    "def _get_thread_access(db):\n",
    "    acc = getattr(_tlocal, \"access\", None)\n",
    "    if acc is None:\n",
    "        try:\n",
    "            acc = db.createAccess()\n",
    "        except Exception:\n",
    "            acc = None\n",
    "        _tlocal.access = acc\n",
    "    return acc\n",
    "\n",
    "def gmst_hours_block_threaded(t0: int, n_hours: int, max_threads: int = 4) -> list[float]:\n",
    "    db, _ = _get_db_and_access()\n",
    "\n",
    "    def read_reduce(t: int) -> float:\n",
    "        acc = _get_thread_access(db)\n",
    "        a = db.read(time=t, quality=QUALITY, access=acc) if acc is not None else db.read(time=t, quality=QUALITY)\n",
    "        return weighted_global_mean_fast(a)\n",
    "\n",
    "    out = [None] * n_hours\n",
    "    with ThreadPoolExecutor(max_workers=max_threads) as ex:\n",
    "        futs = {ex.submit(read_reduce, t0 + k): k for k in range(n_hours)}\n",
    "        for f in as_completed(futs):\n",
    "            out[futs[f]] = f.result()\n",
    "    return out\n",
    "\n",
    "YEAR = 1949\n",
    "start_idx = hour_index_from_iso_big(f\"{YEAR}-01-01 00\")\n",
    "end_idx   = hour_index_from_iso_big(f\"{YEAR+1}-01-01 00\")\n",
    "total_hours = end_idx - start_idx\n",
    "\n",
    "_init_weights_for_quality(start_idx)\n",
    "\n",
    "BLOCK_HOURS = 24 * 60 \n",
    "n_blocks = (total_hours + BLOCK_HOURS - 1) // BLOCK_HOURS\n",
    "\n",
    "tasks = []\n",
    "for b in range(n_blocks):\n",
    "    b_start = start_idx + b * BLOCK_HOURS\n",
    "    b_n = min(BLOCK_HOURS, total_hours - b * BLOCK_HOURS)\n",
    "    tasks.append(delayed(gmst_hours_block_threaded)(b_start, b_n, max_threads=4))  # 4–6 threads works well\n",
    "\n",
    "blocks = dask.compute(*tasks)\n",
    "gmst_vals = np.fromiter((v for block in blocks for v in block), dtype=np.float32, count=total_hours)\n",
    "print(\"Annual GMST\", YEAR, \":\", float(np.nanmean(gmst_vals)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff5eee-0d61-452a-8cd9-4128575a436d",
   "metadata": {},
   "source": [
    "## GMST functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7889c1c-c005-4f9b-ad2f-6d431b6f3abd",
   "metadata": {},
   "source": [
    "## Load data and compute GMST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbd724-f4ea-45e9-9cd1-080bd8046ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import dask\n",
    "# from dask import delayed\n",
    "# import OpenVisus as ov\n",
    "\n",
    "# IDX_PATH = \"/glade/work/dpanta/era5/idx/2T/era5_sfc_2T_zip.idx\"\n",
    "# ov.LoadDataset(IDX_PATH)\n",
    "# H, W = 721, 1440\n",
    "\n",
    "# lat = np.linspace(90.0, -90.0, H, dtype=np.float64)\n",
    "# W1D = np.cos(np.deg2rad(lat)).astype(np.float64)\n",
    "# W2D = np.repeat(W1D[:, None], W, axis=1)  # shape (H, W)\n",
    "\n",
    "# _DB = None\n",
    "# def _get_db():\n",
    "#     global _DB\n",
    "#     if _DB is None:\n",
    "#         _DB = ov.LoadDataset(IDX_PATH)\n",
    "#     return _DB\n",
    "\n",
    "# def weighted_global_mean(arr2d: np.ndarray) -> float:\n",
    "#     a = np.asarray(arr2d, dtype=np.float64)\n",
    "#     bad = (~np.isfinite(a))\n",
    "#     if bad.any():\n",
    "#         a = a.copy()\n",
    "#         a[bad] = np.nan\n",
    "#     valid = np.isfinite(a)\n",
    "#     num = np.nansum(a[valid] * W2D[valid])\n",
    "#     den = np.sum(W2D[valid])\n",
    "#     return float(num / den) if den > 0 else np.nan\n",
    "\n",
    "# def gmst_hours_block(t_start_inclusive: int, n_hours: int) -> list[float]:\n",
    "#     db = _get_db()\n",
    "#     out = []\n",
    "#     for k in range(n_hours):\n",
    "#         a = db.read(time=t_start_inclusive + k)\n",
    "#         out.append(weighted_global_mean(a))\n",
    "#     return out\n",
    "\n",
    "# YEAR = 1945\n",
    "# start_idx = hour_index_from_iso_big(f\"{YEAR}-01-01 00\")\n",
    "# end_idx   = hour_index_from_iso_big(f\"{YEAR+1}-01-01 00\")\n",
    "# total_hours = end_idx - start_idx\n",
    "\n",
    "# time_index_dt = pd.date_range(f\"{YEAR}-01-01 00:00:00\", periods=total_hours, freq=\"H\", tz=\"UTC\")\n",
    "\n",
    "# tasks = [delayed(gmst_hours_block)(start_idx + d*24, 24) for d in range(total_hours // 24)]\n",
    "# blocks = dask.compute(*tasks)\n",
    "# gmst_vals = np.fromiter((v for block in blocks for v in block), dtype=np.float64, count=total_hours)\n",
    "\n",
    "# print(\"Annual GMST 1950:\", np.nanmean(gmst_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bc2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab740a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fc81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conversion-src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
